# Week 5: Structured Data and Metadata Enhancement

## Notebooks Overview

### 1. Generate Dataset.ipynb
This notebook demonstrates how to extract structured metadata from product images and descriptions.

**What You'll Do:**
- Define a product taxonomy in YAML format (categories, subcategories, attributes)
- Analyze clothing images from the irow/ClothingControlV2 dataset
- Use multimodal LLMs (GPT-4o) to identify clothing items and their attributes
- Extract structured product metadata (material, occasion, size, etc.)
- Implement Pydantic validation to ensure metadata conforms to taxonomy
- Upload the generated dataset to Hugging Face for sharing

You'll learn how to consistently extract structured metadata that enhances retrieval capabilities.

### 2. Metadata Filtering.ipynb
This notebook shows how to combine semantic search with structured metadata filtering.

**What You'll Do:**
- Ingest product data into LanceDB with embedded metadata
- Implement query understanding to extract filtering criteria
- Compare three retrieval approaches:
  1. Pure semantic search
  2. Pure metadata filtering
  3. Combined approach (vector + metadata)
- Evaluate performance using recall and MRR metrics
- Demonstrate how metadata filtering handles queries semantic search struggles with
- Improve product descriptions for better retrieval

You'll see how structured metadata dramatically improves retrieval for queries with specific requirements.

### 3. Text-2-SQL.ipynb
This notebook explores integrating database queries into RAG systems.

**What You'll Do:**
- Set up a SQLite database with product inventory and user data
- Generate synthetic user profiles and order history with Faker
- Create stock levels for products
- Implement safe SQL generation using read-only connections
- Build a system that determines when SQL queries are needed vs. retrieval
- Combine retrieval results with database query results
- Test the system with complex queries requiring both approaches

You'll learn how to safely integrate SQL generation to handle queries that require database access.

### 4. PDF-Parser.ipynb
This notebook focuses on extracting structured information from PDF documents with citation capabilities.

**What You'll Do:**
- Process PDF documents to extract text and layout information
- Implement bounding box tracking for visual citations
- Extract structured data from invoices and documents
- Validate extracted information against predefined schemas
- Create systems that can reference specific regions in source documents
- Handle complex PDF structures with tables and forms

You'll build a system that can extract structured information while maintaining citation integrity.

## Data Files and Assets
- `taxonomy.yml`: Product categories, attributes, and valid values
- `init.sql`: Database schema for product inventory and orders
- `data/invoice.pdf`: Sample invoice for PDF parsing
- `helpers.py`: Utility functions for taxonomy processing and validation

## Technical Requirements
- Required libraries: instructor, pydantic, lancedb, openai, sqlite3, faker, poppler, docling
- OpenAI API access (GPT-4o, GPT-4o-mini)
- Basic understanding of SQL and databases
- Familiarity with PDF document structure

## Why These Notebooks Matter
These notebooks move beyond simple semantic search to address complex real-world queries:

1. Queries with specific attribute requirements ("red sneakers under $100")
2. Questions requiring database access ("How many orders did user X place last month?")
3. Document information extraction with citation needs

By combining vector search with structured filtering, SQL access, and document parsing, you'll build RAG systems capable of handling sophisticated user needs that pure semantic search can't address alone.